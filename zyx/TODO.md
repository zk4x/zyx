- [ ] backends
  - [ ] cuda
    - [x] fix async memcopy
    - [ ] tensor cores
    - [ ] fix load calculation, probably using Atomic usize
    - [x] fix event memory leaks, all events must be properly destroyed
    - [ ] channel for context, because cuda context is thread local
    - [ ] rewrite PTX compiler to have proper register manager that handles constants and everything
  - [ ] hip
  - [x] opencl
    - [ ] fix load calculation, probably using Atomic usize
    - [ ] channel for context is required by some (e.g. cuda), but not others
  - [ ] vulkan
    - [ ] initialization
    - [ ] memory management
    - [ ] spirv compiler (to spirv binary)
    - [ ] kernel launch
  - [x] wgpu
    - [ ] fix load calculation, probably using Atomic usize
    - [ ] spirv compiler
  - [x] dummy
    - [ ] validation for program ids
- [ ] runtime
  - [x] fix event handling
  - [x] node deallocation after realization
  - [ ] static graphs - unfortunately necessary for very high performance networks to achieve hundreds of millions of tensor ops/second
- [x] autograd
  - [x] fix t6 test
  - [x] proper backprop, since now we don't quite need to calculate requires_grad_nodes, those are now in gradient_tape
  - [x] fix realize function with gradient tape
- [ ] dtype
  - [ ] quantized dtypes
  - [x] optional implicit dtype casts
- [x] view
  - [x] split on padded view
  - [x] view padding to ir
    - [x] offset
    - [x] padding condition
  - [x] reshaped view to ir
  - [x] axis merging
  - [x] axes reshape
- [ ] kernelizer
  - [x] all dim reduce
  - [x] cache Map<(Kernel, Optimizations), Program> instead of Map<IRKernel, Program>
  - [ ] improve reshape node
    - [x] merges, splits, reshapes of non reduce axes
    - [ ] inserting new loops to the end of the kernel
  - [ ] improve expand node (should almost never store)
  - [ ] improve permute node (should never store)
  - [ ] improve pad node (should almost never store)
  - [ ] pad could also work even with kernels that store stuff, just pad the store view
  - [ ] binary op synchronization (with dependent loads and stores)
  - [x] expand reduce bug
  - [x] fix is expandable conditions
  - [ ] tests for fusion, test will create it's own graph and check how the fused kernel looks
    - [ ] softmax fusion test (eventually should be single kernel)
    - [ ] just asserts that various graphs fuse into single kernel
  - [x] scheduling to multiple devices
  - [x] fix bug when running phi3, panic on min_kernel function
  - [ ] automatic sharding across devices
  - [ ] automatic dropping of unneeded tensors
- [ ] kernel
  - [x] default optimizations
  - [x] indexing for padded views
  - [x] indexing for multi reshape views
  - [x] common subexpression elimination
  - [x] dead store elimination
  - [x] kernel flops, memory reads, memory writes
  - [ ] global to inner loop splitting
  - [ ] inner loop splitting
  - [ ] loop reordering
  - [ ] loop unrolling
    - [ ] in optimizer
    - [x] in kernel
  - [ ] loop invariant code motion
  - [ ] vectorization, vector dtypes
  - [ ] tensor cores/tiling
  - [ ] merge all mul + add into mad instructions
  - [ ] local tiling of all variables
  - [ ] streaming dual reduce ops (e.g. streaming softmax)
  - [ ] flash attention
  - [x] optimizer with search
- [ ] testing
  - [ ] fuzzy tester
    - [x] unary ops
    - [ ] movemnt ops
    - [ ] binary ops
  - [x] pad_2
  - [x] reshape_permute_1
  - [x] rope_1
  - [x] rope_2
  - [x] softmax_1
  - [x] padding on elementwise kernel
  - [x] expand on elementwise kernel
  - [x] reshape on elementwise kernel
  - [x] permute on elementwise kernel
  - [x] padding on reduce kernel
  - [x] expand on reduce kernel
  - [x] reshape on reduce kernel
  - [x] permute on reduce kernel
  - [ ] lot of testing for kernelizer correctness
  - [ ] more autograd tests
- [ ] tensor
  - [ ] gather
  - [ ] scatter
  - [ ] solve
  - [ ] inverse of matrix
  - [ ] pinverse
  - [ ] eigvalsh
  - [ ] singular value decomposition
  - [ ] instance norm
  - [ ] interpolate
  - [ ] upsample
  - [ ] downsample
  - [ ] erf
  - [ ] erfinv
  - [ ] lgamma (log gamma)
  - [ ] i0 (modified Bessel function)
  - [ ] trunc
  - [ ] frac
  - [ ] ceil
  - [ ] round
  - [ ] nll loss
  - [ ] bce loss
  - [ ] huber loss
  - [ ] smooth l1 loss
  - [ ] ctc loss
  - [ ] triplet margin loss
  - [ ] frobenius norm
  - [ ] spectral norm
  - [x] tril
  - [x] triu

- [x] docs
  - [x] manual for adding new backends
- [x] dependencies
  - [x] replace serde with nanoserde
  - [x] implement custom progress bar
  - [x] remove indicatiff
  - [x] remove xdg
  - [x] remove rand

- examples
  - [ ] get phi working
    - [ ] fix tensor memory leak
