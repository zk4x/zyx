- [ ] backends
  - [ ] cuda
    - [x] fix async memcopy
    - [ ] tensor cores
    - [ ] fix load calculation, probably using Atomic usize
    - [x] fix event memory leaks, all events must be properly destroyed
  - [ ] hip
  - [x] opencl
    - [ ] fix load calculation, probably using Atomic usize
  - [ ] vulkan
    - [ ] initialization
    - [ ] memory management
    - [ ] spirv compiler (to spirv binary)
    - [ ] kernel launch
  - [x] wgpu
    - [ ] fix load calculation, probably using Atomic usize
    - [ ] spirv compiler
      - [ ] conversion to spirv SSA (dealing with accumulators)
  - [x] dummy
    - [ ] validation for program ids
- [ ] dtype
  - [ ] quantized dtypes
  - [x] optional implicit dtype casts
- [x] runtime
  - [x] graph size optimization - remove axes from Nodes, put it into map like shapes and dtypes
  - [x] realization while tracing gradients
  - [x] realization while not tracing gradients
  - [x] gradient tape
- [x] scheduler
  - [x] cache Map<(Kernel, Optimizations), Program> instead of Map<IRKernel, Program>
  - [ ] fix reshape node
    - [x] merges, splits, reshapes of non reduce axes
    - [ ] inserting new loops to the end of the kernel
  - [ ] pad should also work even with kernels that store stuff, just pad the store view
  - [x] expand reduce bug
  - [x] fix is expandable conditions
  - [ ] tests for fusion, test will create it's own graph and check how the fused kernel looks
    - [ ] softmax fusion test (eventually should be single kernel)
    - [ ] just asserts that various graphs fuse into single kernel
  - [x] scheduling to multiple devices
  - [ ] automatic sharding across devices
  - [x] fix bug when running phi3, panic on min_kernel function
- [x] kernel
  - [x] ops remove unary view
  - [x] ops remove binary views
- [ ] optimizer
  - [x] default optimizations
  - [ ] register tiling of all variables
  - [ ] local tiling of all variables
  - [ ] better picking of next optimization, or even optimization search
  - [ ] flash attention
  - [ ] splitting of global loops into register loops
  - [ ] splitting of register loops into global loops (if global work size is too small)
- [x] view
  - [x] split on padded view
  - [x] view padding to ir
    - [x] offset
    - [x] padding condition
  - [x] reshaped view to ir
  - [x] axis merging
  - [x] axes reshape
- [ ] IR
  - [x] add dtype to load vop, so that we don't need to pass graph to ir
  - [x] do not pass graph to ir
  - [x] change ir register id to u16
  - [x] remove ref counting from ir
  - [x] merge all mul + add into mad instructions
  - [x] add new reference counting that accounts for all variables, including indexing variables
  - [x] loop invariant code motion
  - [x] fix destructuring back from SSA
  - [x] loop unrolling
  - [ ] loop splitting!
  - [ ] loop reordering!
  - [x] constant folding and propagation
  - [ ] common subexpression elimination
  - [x] dead store elimination
  - [ ] vectorization, vector dtypes!
  - [ ] ops fusion, merges1
- [ ] runtime
  - [x] fix event handling
  - [x] fix node deallocation after realization
  - [ ] graph recording - unfortunatelly seems necessary for some high performance stuff, basically start record at the beginning of the loop and stop recording at the end of the loop. Then there needs to be a detector which tensors are inputs to this graph, which are model's parameters.
- [ ] node
  - [ ] increase the size of nodes, by adding more complex ops, e.g. tanh. These should still only be unary, binary, or reduce ops, not new types of ops. It will keep the graph smaller and faster to kernelize and in IR we can split them into smaller ops again to keep backends simple. But we will have to see if that is actually a good idea.
  - [ ] implement lowerer in the beginning of IRKernel
  - [ ] add tanh
  - [ ] add dot
- [ ] backward
  - [ ] fix t6 test
  - [ ] more backpropagation tests
- [ ] testing
  - [ ] lot of testing for scheduler correctness
  - [ ] fuzzy tester
    - [ ] movemnt ops
    - [ ] binary ops
- [ ] autograd
  - [ ] drop unneded nodes when gradient tape is released
  - [ ] proper realize function with gradient tape
  - [x] proper backprop, since now we don't quite need to calculate requires_grad_nodes, those are now in gradient_tape

- [x] docs
  - [x] manual for adding new backends
- [x] dependencies
  - [x] replace serde with nanoserde
  - [ ] implement custom progress bar
  - [ ] remove indicatiff

- examples
  - [x] get phi working
    - [ ] fix tensor memory leak
