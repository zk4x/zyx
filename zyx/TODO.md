- [ ] backends
  - [ ] cuda
    - [x] fix async memcopy
    - [ ] tensor cores
    - [ ] fix load calculation, probably using Atomic usize
    - [x] fix event memory leaks, all events must be properly destroyed
    - [ ] channel for context, because cuda context is thread local
  - [ ] hip
  - [x] opencl
    - [ ] fix load calculation, probably using Atomic usize
    - [ ] channel for context is required by some (e.g. cuda), but not others
  - [ ] vulkan
    - [ ] initialization
    - [ ] memory management
    - [ ] spirv compiler (to spirv binary)
    - [ ] kernel launch
  - [x] wgpu
    - [ ] fix load calculation, probably using Atomic usize
    - [ ] spirv compiler
  - [x] dummy
    - [ ] validation for program ids
- [x] runtime
  - [x] fix event handling
  - [ ] node deallocation after realization
  - [ ] static graphs - unfortunately necessary for very high performance networks to achieve millions of tensor ops/second
- [x] autograd
  - [x] fix t6 test
  - [x] proper backprop, since now we don't quite need to calculate requires_grad_nodes, those are now in gradient_tape
  - [ ] fix realize function with gradient tape
- [ ] dtype
  - [ ] quantized dtypes
  - [x] optional implicit dtype casts
- [x] view
  - [x] split on padded view
  - [x] view padding to ir
    - [x] offset
    - [x] padding condition
  - [x] reshaped view to ir
  - [x] axis merging
  - [x] axes reshape
- [x] kernelizer
  - [x] all dim reduce
  - [ ] kernel reshape with shape that contains reduce ops and add new loops after those
  - [x] cache Map<(Kernel, Optimizations), Program> instead of Map<IRKernel, Program>
  - [ ] improve reshape node
    - [x] merges, splits, reshapes of non reduce axes
    - [ ] inserting new loops to the end of the kernel
  - [ ] improve expand node (should almost never store)
  - [ ] improve permute node (should never store)
  - [ ] improve pad node (should almost never store)
  - [ ] pad could also work even with kernels that store stuff, just pad the store view
  - [x] expand reduce bug
  - [x] fix is expandable conditions
  - [ ] tests for fusion, test will create it's own graph and check how the fused kernel looks
    - [ ] softmax fusion test (eventually should be single kernel)
    - [ ] just asserts that various graphs fuse into single kernel
  - [x] scheduling to multiple devices
  - [x] fix bug when running phi3, panic on min_kernel function
  - [ ] automatic sharding across devices
  - [ ] automatic dropping of unneeded tensors
- [x] kernel
  - [x] default optimizations
  - [x] indexing for padded views
  - [x] indexing for multi reshape views
  - [x] common subexpression elimination
  - [x] dead store elimination
  - [x] kernel flops, memory reads, memory writes
  - [ ] global to inner loop splitting
  - [ ] inner loop splitting
  - [ ] loop reordering
  - [ ] loop unrolling
  - [ ] loop invariant code motion
  - [ ] vectorization, vector dtypes
  - [ ] tensor cores/tiling
  - [ ] merge all mul + add into mad instructions
  - [ ] local tiling of all variables
  - [ ] streaming dual reduce ops (e.g. streaming softmax)
  - [ ] flash attention
  - [x] optimizer with search
- [ ] testing
  - [ ] fuzzy tester
    - [x] unary ops
    - [ ] movemnt ops
    - [ ] binary ops
  - [x] pad_2
  - [x] reshape_permute_1
  - [x] rope_2
  - [ ] softmax_1
  - [ ] padding on elementwise kernel
  - [ ] expand on elementwise kernel
  - [ ] reshape on elementwise kernel
  - [ ] permute on elementwise kernel
  - [ ] padding on reduce kernel
  - [ ] expand on reduce kernel
  - [ ] reshape on reduce kernel
  - [ ] permute on reduce kernel
  - [ ] lot of testing for kernelizer correctness
  - [ ] more autograd tests

- [x] docs
  - [x] manual for adding new backends
- [x] dependencies
  - [x] replace serde with nanoserde
  - [x] implement custom progress bar
  - [x] remove indicatiff
  - [x] remove xdg
  - [x] remove rand

- examples
  - [x] get phi working
    - [ ] fix tensor memory leak
