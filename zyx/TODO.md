- [ ] tests for fusion in generate kernels, test will create it's own graph and check how the fused kernel looks
    - [ ] softmax fusion test (eventually should be single kernel)
- [x] view
  - [x] split on padded view
  - [x] view padding to ir
    - [x] offset
    - [x] padding condition
  - [x] reshaped view to ir
  - [x] axis merging
  - [x] axes reshape
- [ ] ir rewrite
  - [x] add dtype to load vop, so that we don't need to pass graph to ir
  - [x] do not pass graph to ir
  - [x] change ir register id to u16
  - [x] remove ref counting from ir
  - [x] merge all mul + add into mad instructions
  - [x] add new reference counting that accounts for all variables, including indexing variables
  - [ ] deduplicate all calculations
  - [ ] expression folding, optimizations for ops, merges
  - [ ] loop invariant code motion
  - [ ] dead code elimination
  - [ ] loop unrolling
  - [ ] vector dtypes
  - [ ] constant evaluation
- [x] vops remove unary view
- [x] vops remove binary views
- [x] manual for adding new backends
- [ ] scheduler upgrades
  - [ ] fix reshape node
    - [x] merges, splits, reshapes of non reduce axes
    - [ ] inserting new loops to the end of the kernel
  - [ ] pad should also work even with kernels that store stuff, just pad the store view
  - [ ] expand reduce bug
- [ ] backends
  - [ ] cuda
    - [ ] fix async memcopy
  - [ ] hip
  - [x] opencl
  - [ ] vulkan
  - [ ] wgpu
  - [ ] dummy
- [ ] use stable-vec instead of index_map? But what about node reuse order?
- [x] cache Map<(Kernel, Optimizations), Program> instead of Map<IRKernel, Program>
- [ ] register tiling of all variables
- [ ] local tiling of all variables
- [ ] get phi working
- [ ] tensor cores support
- [ ] flash attention
- [x] graph size optimization - remove axes from Nodes, put it into map like shapes and dtypes
- [x] replace serde with nanoserde
